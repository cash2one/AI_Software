{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrom the command line you can convert a notebook to python with this command:\\n\\nipython nbconvert --to python <YourNotebook>.ipynb\\n\\nYou may have to install the python mistune package:\\n\\nsudo pip install mistune\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "import Video_Utils\n",
    "import CNN_Utils\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Convolution2D, UpSampling2D, MaxPooling2D, ZeroPadding2D, Reshape, merge\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda, Merge\n",
    "from keras.engine import Layer\n",
    "\n",
    "'''\n",
    "From the command line you can convert a notebook to python with this command:\n",
    "\n",
    "ipython nbconvert --to python <YourNotebook>.ipynb\n",
    "\n",
    "You may have to install the python mistune package:\n",
    "\n",
    "sudo pip install mistune\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################## GLOBAL VARIABLES #########################################\n",
    "# These variables are specific to the dataset of the problem. \n",
    "# Our data is the following: \n",
    "#\n",
    "# time-series of 4 cameras (128 x 128 x 3) looking at a robot complete a task (pick up object, place in bin)\n",
    "# + corresponding time series of 7-d vector of joint commands (6 DOF + gripper) denoting the position delta command.\n",
    "#\n",
    "# In future comments, we will refer to the states and actions as x(t) and u(t) respectively, where 'state' \n",
    "# denotes the visual data sensed from the world (the 4 cameras) and action is the 7-d joint position delta vector. \n",
    "#\n",
    "# Data was generated and collected using the V-REP (http://www.coppeliarobotics.com/) robot simulator \n",
    "# (free/education version). Please contact the author of this notebook for the specific scene and data-generation\n",
    "# script \n",
    "\n",
    "global CAM_W\n",
    "global CAM_H\n",
    "global CAM_C\n",
    "global NUM_CAM_CHANNELS\n",
    "global NUM_CAMS\n",
    "global ACTION_LEN\n",
    "\n",
    "CAM_W = 128\n",
    "CAM_H = 128\n",
    "CAM_C = 3\n",
    "NUM_CAM_CHANNELS = 3\n",
    "NUM_CAMS = 4\n",
    "CAM_C = NUM_CAM_CHANNELS*NUM_CAMS #R,G,B x 4 cameras\n",
    "ACTION_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################### MODEL PARAMETERS #########################################\n",
    "global NUM_VGG_FEAT_MAPS\n",
    "global NUM_FUTURE_FRAMES\n",
    "global NUM_PAST_FRAMES\n",
    "global VGG_FEAT_W\n",
    "global VGG_FEAT_H \n",
    "global CONV1_FEAT_MAPS\n",
    "global CONV2_FEAT_MAPS\n",
    "global CONV3_FEAT_MAPS\n",
    "global CONVP1_FEAT_MAPS \n",
    "global CONVP2_FEAT_MAPS\n",
    "global CONVP3_FEAT_MAPS\n",
    "global CONVF1_FEAT_MAPS\n",
    "global CONVF2_FEAT_MAPS\n",
    "global CONVF3_FEAT_MAPS\n",
    "global CONVD1_FEAT_MAPS\n",
    "global CONVD2_FEAT_MAPS\n",
    "global CONVD3_FEAT_MAPS\n",
    "global D_DENSE1\n",
    "global D_DENSE2\n",
    "global D_DENSE3\n",
    "\n",
    "VGG_FEAT_W = 64        # using \"block1_pool\"\n",
    "VGG_FEAT_H = 64        # using \"block1_pool\"  \n",
    "NUM_VGG_FEAT_MAPS = 64 # using \"block1_pool\" (per camera)\n",
    "NUM_FUTURE_FRAMES = 10\n",
    "NUM_PAST_FRAMES = 10\n",
    "\n",
    "CONVP1_FEAT_MAPS = round(NUM_VGG_FEAT_MAPS*NUM_PAST_FRAMES/2) \n",
    "CONVP2_FEAT_MAPS = round(CONVP1_FEAT_MAPS/2) \n",
    "CONVP3_FEAT_MAPS = round(CONVP2_FEAT_MAPS/2)  \n",
    "\n",
    "CONV1_FEAT_MAPS = round(4*NUM_PAST_FRAMES)\n",
    "CONV2_FEAT_MAPS = round(2*NUM_PAST_FRAMES)\n",
    "CONV3_FEAT_MAPS = NUM_PAST_FRAMES\n",
    "\n",
    "CONVF4_FEAT_MAPS = round(CAM_C*NUM_CAM_CHANNELS)\n",
    "CONVF3_FEAT_MAPS = round(CONVF4_FEAT_MAPS*2)\n",
    "CONVF2_FEAT_MAPS = round(CONVF3_FEAT_MAPS*2)    # e.g. 48\n",
    "CONVF1_FEAT_MAPS = CONVF2_FEAT_MAPS             # e.g. 48\n",
    "\n",
    "D_DENSE3 = VGG_FEAT_W * VGG_FEAT_H\n",
    "D_DENSE2 = round(D_DENSE3 / 4)\n",
    "D_DENSE1 = round(D_DENSE2 / 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG model loaded. Time elasped: 3.1 secs\n"
     ]
    }
   ],
   "source": [
    "################################################# THE MODEL ####################################################\n",
    "'''\n",
    "An explanation of the dynamics model and the buffers: \n",
    "\n",
    "We are training a model to produce p(x(t+1: t+F) | x(t-P: t), u(t-P: t), u(t+1: t+F-1)) \n",
    "\n",
    "I.e.: predict the future F frames, \n",
    "      given the past P frames, past P actions, and the future F-1 actions \n",
    "      \n",
    "This can be viewed as unrolling the classic p(x(t+1) | x(t), u(t)) dynamics model. \n",
    "\n",
    "We have proposed this 'unrolled dynamics' problem formulation for the following reasons:\n",
    "\n",
    "1) Conditioning on longer sequences to capture the effects of repeated actions, reduce discretization errors\n",
    "2) Improved training due to longer sequences. Single-step visual models have a tendency to \"copy\" prev. frame\n",
    "   as the output\n",
    "3) Be able to visualize a future state trajectory based on a proposed set of actions.\n",
    "   ---> this has applications in control planning, and can be useful to a human operator/co-worker, or for\n",
    "        another system to check for safety hazards, etc. Plus makes a sick demo (\"this is what the robot is\n",
    "        thinking\") lol. \n",
    "        \n",
    "PREV_FRAMES_BUFFER ---> input, this is x(t-P: t)         (size P,W,H,3*num_cams)\n",
    "PREV_ACTION_BUFFER ---> input, this is u(t-P: t)         (size P,A)\n",
    "FUTURE_FRAMES_BUFFER ---> output, this is x(t+1: t+F)    (size F,W,H,3*num_cams)\n",
    "FUTURE_ACTION_BUFFER ---> input, this is u(t+1: t+F-1)   (size F-1,A)\n",
    "\n",
    "(note: visual inputs may be pre-processed with another pre-trained model)\n",
    "'''\n",
    "# Use part of pre-trained VGG model to seed features with reasonable features: (used online during training)  \n",
    "vgg_preprocessor = CNN_Utils.GetVGGModel(\"block1_pool\", CAM_W, CAM_H, print_timing=1) \n",
    "\n",
    "# Model input #1: past frames\n",
    "input_prev_frames = Input(shape=(VGG_FEAT_W, VGG_FEAT_H, NUM_VGG_FEAT_MAPS*NUM_PAST_FRAMES), name='input_prev_frames_raw')\n",
    "# below: some layers to learn what information is important from the past\n",
    "input_prev_frames = Convolution2D(CONVP1_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames)\n",
    "input_prev_frames = Convolution2D(CONVP2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames)\n",
    "input_prev_frames = Convolution2D(CONVP3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames) \n",
    "\n",
    "# Model input #1: past actions \n",
    "input_prev_actions_raw = Input(shape=(NUM_PAST_FRAMES, ACTION_LEN), name='input_prev_actions_raw')\n",
    "# Below: project and reshape, so we can merge inputs later: \n",
    "input_prev_actions_p = Dense(output_dim=VGG_FEAT_W*VGG_FEAT_H*1, activation='elu')(input_prev_actions_raw)\n",
    "input_prev_actions_p_r = Reshape((VGG_FEAT_W, VGG_FEAT_H, 1), name='input_prev_actions_p_r')(input_prev_actions_p)\n",
    "\n",
    "# Model input #3: future actions\n",
    "input_future_actions_raw = Input(shape=(NUM_PAST_FRAMES, ACTION_LEN), name='input_future_actions_raw')\n",
    "# format for convenience, to let us 'pick off' actions and sequentially predict the next frame logically\n",
    "# Below: these parameters gather information for an action pertaining to how it affects a future state\n",
    "D_A1 = Dense(output_dim=D_DENSE1, activation='elu', name='D_A1')(input_future_actions_raw)\n",
    "D_A2 = Dense(output_dim=D_DENSE2, activation='elu', name='D_A2')(D_A1)\n",
    "D_A3 = Dense(output_dim=D_DENSE3, activation='elu', name='D_A3')(D_A2)\n",
    "future_action_branch = Reshape((VGG_FEAT_W, VGG_FEAT_H, 1), name='action_branch_r')(D_A3)\n",
    "\n",
    "# MERGE PAST INPUTS: \n",
    "merged_input = merge([input_prev_frames, input_prev_actions_p_r, future_action_branch], \n",
    "                      mode='concat', concat_axis=3, name='merged_prev_inputs')\n",
    "# ^ so now past frames, info from past actions has been merged into a tensor that is size e.g.:\n",
    "# [VGG_W, VGG_H, (VGG_feat_maps * NUM_PAST_FRAMES)/8 + 2] (e.g. == 82 for 10 past frames)\n",
    "\n",
    "# Pre-process merged inputs that contain past information:\n",
    "# This is meant to form a more efficient representation to be used in predicting future frames sequentially given actions:\n",
    "merged_input = Convolution2D(CONV1_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='merged_lvl_1')(merged_input)\n",
    "merged_input = Convolution2D(CONV2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='merged_lvl_2')(merged_input)\n",
    "merged_input = Convolution2D(CONV3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='merged_lvl_3')(merged_input)\n",
    "\n",
    "# Shared conv (and projected & reshaped action) layers for predicting the next frame given a new action: \n",
    "SHARED_C1 = Convolution2D(CONVF1_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='SHARED_C1')\n",
    "SHARED_C2 = Convolution2D(CONVF2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='SHARED_C2')\n",
    "SHARED_C3 = Convolution2D(CONVF3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='SHARED_C3')\n",
    "SHARED_C4 = Convolution2D(CONVF4_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='SHARED_C4')\n",
    "# ^ SHARED_C4 is a final output layer, spits out a frame (defined as WxHx3*num_cams)\n",
    "\n",
    "################################# WHAT ARE WE DOING HERE????????????\n",
    "\n",
    "nfp = SHARED_C1(merged_data)\n",
    "nfp = SHARED_C2(nfp)\n",
    "nfp = SHARED_C3(nfp)\n",
    "nfp = SHARED_C4(nfp) # and this is a frame predicted from the past info, and proposed future action\n",
    "\n",
    "# SEQUENTIAL PREDICTION: \n",
    "# Use the shared layers, sequential actions to predict next frame\n",
    "for t in range(1, NUM_FUTURE_FRAMES):\n",
    "    \n",
    "    \n",
    "    nfp = SHARED_C1(merged_data)\n",
    "    nfp = SHARED_C2(nfp)\n",
    "    nfp = SHARED_C3(nfp)\n",
    "    nfp = SHARED_C4(nfp) # and this is a frame predicted from the past info, and proposed future action\n",
    "    \n",
    "    next_frames = merge([next_frames, nfp], mode='concat', concat_axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### INITIALIZATION, SETUP OF MODEL #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################## Globals for training params ####################################\n",
    "\n",
    "global TOTAL_TRAINING_ITRS\n",
    "global SAVE_CHECKPOINT_ITRS\n",
    "global NUM_DEMONSTRATIONS\n",
    "global CURR_DEMONSTRATION\n",
    "global LENGTH_CURR_DEMONSTRATION # e.g. current demo we're looking at is 230 timesteps\n",
    "global T_CURR_DEMONSTRATION      # and e.g. we're currently on timestep 87\n",
    "global PERCENT_TRAIN             # percent of data used for training vs. valudation\n",
    "global DEMONSTRATION_FOLDERS\n",
    "global TRAINING_FOLDERS\n",
    "global TESTING_FOLDERS\n",
    "global NUM_TRAINING_DEMONSTRATIONS\n",
    "global NUM_TESTING_DEMONSTRATIONS\n",
    "\n",
    "global PREV_FRAMES_BUFFER\n",
    "global PREV_ACTION_BUFFER\n",
    "global FUTURE_FRAMES_BUFFER\n",
    "global FUTURE_ACTION_BUFFER\n",
    "\n",
    "PREV_FRAMES_BUFFER = np.zeros((NUM_PAST_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "PREV_ACTION_BUFFER = np.zeros((NUM_PAST_FRAMES, ACTION_LEN))\n",
    "FUTURE_FRAMES_BUFFER = np.zeros((NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "FUTURE_ACTION_BUFFER = np.zeros((NUM_FUTURE_FRAMES-1, ACTION_LEN))\n",
    "\n",
    "TOTAL_TRAINING_ITRS = 100000\n",
    "SAVE_CHECKPOINT_ITRS = 100\n",
    "CURR_DEMONSTRATION = -1 # start on the first one, loop to another one each itr\n",
    "LENGTH_CURR_DEMONSTRATION = -1\n",
    "PERCENT_TRAIN = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running program...\n",
      "u_sequence_1486408000621\n",
      "u_sequence_1486420877768\n",
      "u_sequence_1486421372155\n",
      "u_sequence_1486422003939\n",
      "u_sequence_1486423780289\n",
      "u_sequence_1486424226636\n",
      "u_sequence_1486424664609\n",
      "u_sequence_1486425113072\n",
      "u_sequence_1486425723665\n",
      "u_sequence_1486426394928\n",
      "u_sequence_1486483126414\n",
      "u_sequence_1486483372904\n",
      "u_sequence_1486483607589\n",
      "u_sequence_1486483842933\n",
      "u_sequence_1486484146973\n",
      "u_sequence_1486484663068\n",
      "u_sequence_1486485509467\n",
      "u_sequence_1486485832231\n",
      "u_sequence_1486486076841\n",
      "u_sequence_1486486803219\n",
      "15\n",
      "5\n",
      "20\n",
      "\n",
      "===== Training on demonstration #:0 with num timesteps 185\n",
      "\n",
      "===== Training on demonstration #:1 with num timesteps 169\n",
      "\n",
      "===== Training on demonstration #:13 with num timesteps 226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2c21dfd103ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mCURR_DEMONSTRATION\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNUM_TRAINING_DEMONSTRATIONS\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideo_Utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadFramesActionsFromFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAINING_FOLDERS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCURR_DEMONSTRATION\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCAM_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCAM_H\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCAM_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[1;31m# ^ about 5 secs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################## START THE TRAINING #######################################\n",
    "\n",
    "print(\"Running program...\")\n",
    "\n",
    "DEMONSTRATION_FOLDERS = Video_Utils.GetFoldersForRuns()\n",
    "\n",
    "NUM_DEMONSTRATIONS = len(DEMONSTRATION_FOLDERS)\n",
    "\n",
    "for f in DEMONSTRATION_FOLDERS:\n",
    "    print(f)\n",
    "\n",
    "# Separate into training and testing data based on the number of recordings (folders) we have: \n",
    "random.shuffle(DEMONSTRATION_FOLDERS)    \n",
    "lim_separate = round(NUM_DEMONSTRATIONS * PERCENT_TRAIN)    \n",
    "TRAINING_FOLDERS = DEMONSTRATION_FOLDERS[0:lim_separate]\n",
    "TESTING_FOLDERS = DEMONSTRATION_FOLDERS[lim_separate:]\n",
    "NUM_TRAINING_DEMONSTRATIONS = len(TRAINING_FOLDERS)\n",
    "NUM_TESTING_DEMONSTRATIONS = len(TESTING_FOLDERS)\n",
    "\n",
    "print(NUM_TRAINING_DEMONSTRATIONS); print(NUM_TESTING_DEMONSTRATIONS); print(NUM_DEMONSTRATIONS) # a + b = c\n",
    "\n",
    "    \n",
    "\n",
    "# Data flow process: we train on entire folder (sample run of a robot) before moving on to the next to \n",
    "# amortize the time required to load that folder's training data into RAM (multiple seconds). For a dynamics\n",
    "# model this should be perfectly acceptable because the dynamics to be learned are ideally the *same* between\n",
    "# different sample runs recorded on the (simulated) robot. \n",
    "itrs = 0\n",
    "while itrs < TOTAL_TRAINING_ITRS:\n",
    "    \n",
    "    # Choose which expert demonstration we're using: \n",
    "    CURR_DEMONSTRATION = randint(0,NUM_TRAINING_DEMONSTRATIONS-1)\n",
    "\n",
    "    frames, actions = Video_Utils.LoadFramesActionsFromFolder(TRAINING_FOLDERS[CURR_DEMONSTRATION], CAM_W, CAM_H, CAM_C, ACTION_LEN)\n",
    "    # ^ about 5 secs\n",
    "    \n",
    "    LENGTH_CURR_DEMONSTRATION = frames.shape[0] # number of timesteps for this demonstration\n",
    "    \n",
    "    print(\"\\n===== Training on demonstration #:\"+str(CURR_DEMONSTRATION)+\" with num timesteps \"+str(LENGTH_CURR_DEMONSTRATION))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
