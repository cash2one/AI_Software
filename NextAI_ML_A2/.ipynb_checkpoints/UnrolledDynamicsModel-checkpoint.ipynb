{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrom the command line you can convert a notebook to python with this command:\\n\\nipython nbconvert --to python <YourNotebook>.ipynb\\n\\nYou may have to install the python mistune package:\\n\\nsudo pip install mistune\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import cv2\n",
    "sys.path.append('..')\n",
    "import Video_Utils\n",
    "import CNN_Utils\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Convolution2D, UpSampling2D, MaxPooling2D, ZeroPadding2D, Reshape, merge\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda, Merge\n",
    "from keras.engine import Layer\n",
    "\n",
    "'''\n",
    "From the command line you can convert a notebook to python with this command:\n",
    "\n",
    "ipython nbconvert --to python <YourNotebook>.ipynb\n",
    "\n",
    "You may have to install the python mistune package:\n",
    "\n",
    "sudo pip install mistune\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################## GLOBAL VARIABLES #########################################\n",
    "# These variables are specific to the dataset of the problem. \n",
    "# Our data is the following: \n",
    "#\n",
    "# time-series of 4 cameras (128 x 128 x 3) looking at a robot complete a task (pick up object, place in bin)\n",
    "# + corresponding time series of 7-d vector of joint commands (6 DOF + gripper) denoting the position delta command.\n",
    "#\n",
    "# In future comments, we will refer to the states and actions as x(t) and u(t) respectively, where 'state' \n",
    "# denotes the visual data sensed from the world (the 4 cameras) and action is the 7-d joint position delta vector. \n",
    "#\n",
    "# Data was generated and collected using the V-REP (http://www.coppeliarobotics.com/) robot simulator \n",
    "# (free/education version). Please contact the author of this notebook for the specific scene and data-generation\n",
    "# script \n",
    "\n",
    "global CAM_W\n",
    "global CAM_H\n",
    "global CAM_C\n",
    "global NUM_CAMS\n",
    "global ACTION_LEN\n",
    "\n",
    "CAM_W = 128\n",
    "CAM_H = 128\n",
    "CAM_C = 3\n",
    "NUM_CAMS = 4\n",
    "ACTION_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################### MODEL PARAMETERS #########################################\n",
    "global NUM_FUTURE_FRAMES\n",
    "global NUM_PAST_FRAMES\n",
    "#global NUM_VGG_FEAT_MAPS\n",
    "#global VGG_FEAT_W\n",
    "#global VGG_FEAT_H \n",
    "global CONV1_FEAT_MAPS\n",
    "global CONV2_FEAT_MAPS\n",
    "global CONV3_FEAT_MAPS\n",
    "global CONVP1_FEAT_MAPS \n",
    "global CONVP2_FEAT_MAPS\n",
    "global CONVP3_FEAT_MAPS\n",
    "global CONVF1_FEAT_MAPS\n",
    "global CONVF2_FEAT_MAPS\n",
    "global CONVF3_FEAT_MAPS\n",
    "global CONVD1_FEAT_MAPS\n",
    "global CONVD2_FEAT_MAPS\n",
    "global CONVD3_FEAT_MAPS\n",
    "global D_DENSE1\n",
    "global D_DENSE2\n",
    "global D_DENSE3\n",
    "global UDM_model       # The actual Keras model\n",
    "\n",
    "#VGG_FEAT_W = 64        # using \"block1_pool\"\n",
    "#VGG_FEAT_H = 64        # using \"block1_pool\"  \n",
    "#NUM_VGG_FEAT_MAPS = 64 # using \"block1_pool\" (per camera)\n",
    "NUM_FUTURE_FRAMES = 10\n",
    "NUM_PAST_FRAMES = 10\n",
    "# Below: these layers create a more data-efficient representation of past from preprocessed features\n",
    "CONVP1_FEAT_MAPS = round(CAM_C*NUM_CAMS*NUM_PAST_FRAMES/2)        # e.g. 120/2 = 60\n",
    "CONVP2_FEAT_MAPS = round(CONVP1_FEAT_MAPS/2)                      # e.g. 30\n",
    "CONVP3_FEAT_MAPS = round(CONVP2_FEAT_MAPS/2)                      # e.g. 15\n",
    "# Below: these layers create a more data-efficient representation from merged frame,action [from the past] inputs\n",
    "CONV1_FEAT_MAPS = CONVP3_FEAT_MAPS\n",
    "CONV2_FEAT_MAPS = CONVP3_FEAT_MAPS\n",
    "CONV3_FEAT_MAPS = NUM_PAST_FRAMES\n",
    "# Below: these are final output layers that transform merged input data to predicted output frames\n",
    "CONVF1_FEAT_MAPS = (CONV3_FEAT_MAPS + 1)*2                        # e.g. 22\n",
    "CONVF2_FEAT_MAPS = round(CONVF1_FEAT_MAPS*2)                      # e.g. 48\n",
    "CONVF3_FEAT_MAPS = round(CONVF2_FEAT_MAPS*2)                      # e.g. 96\n",
    "CONVF4_FEAT_MAPS = round(CAM_C*NUM_CAMS*NUM_PAST_FRAMES)  # e.g. 3*4*NUM_PAST_FRAMES = 120 \n",
    "# Below: parameters to learn important features in the sequence of future actions \n",
    "D_DENSE3 = CAM_W * CAM_H\n",
    "D_DENSE2 = round(D_DENSE3 / 8)\n",
    "D_DENSE1 = round(D_DENSE2 / 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_prev_frames_raw (InputLaye (None, 128, 128, 120) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_50 (Convolution2D) (None, 128, 128, 60)  64860       input_prev_frames_raw[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "input_prev_actions_raw (InputLay (None, 70)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_51 (Convolution2D) (None, 128, 128, 30)  16230       convolution2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 16384)         1163264     input_prev_actions_raw[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_52 (Convolution2D) (None, 128, 128, 15)  4065        convolution2d_51[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "input_prev_actions_p_r (Reshape) (None, 128, 128, 1)   0           dense_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_future_actions_raw (InputL (None, 63)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merged_prev_inputs (Merge)       (None, 128, 128, 16)  0           convolution2d_52[0][0]           \n",
      "                                                                   input_prev_actions_p_r[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "D_A1 (Dense)                     (None, 256)           16384       input_future_actions_raw[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "merged_lvl_1 (Convolution2D)     (None, 128, 128, 15)  11775       merged_prev_inputs[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "D_A2 (Dense)                     (None, 2048)          526336      D_A1[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "merged_lvl_2 (Convolution2D)     (None, 128, 128, 15)  2040        merged_lvl_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "D_A3 (Dense)                     (None, 16384)         33570816    D_A2[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "merged_lvl_3 (Convolution2D)     (None, 128, 128, 10)  1360        merged_lvl_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "action_branch_r (Reshape)        (None, 128, 128, 1)   0           D_A3[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "merged_efficient_inputs_all (Mer (None, 128, 128, 11)  0           merged_lvl_3[0][0]               \n",
      "                                                                   action_branch_r[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "Out_C1 (Convolution2D)           (None, 128, 128, 22)  11880       merged_efficient_inputs_all[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "Out_C2 (Convolution2D)           (None, 128, 128, 44)  8756        Out_C1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "Out_C3 (Convolution2D)           (None, 128, 128, 88)  34936       Out_C2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "Out_C4 (Convolution2D)           (None, 128, 128, 120) 95160       Out_C3[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 35,527,862\n",
      "Trainable params: 35,527,862\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "################################################# THE MODEL ####################################################\n",
    "'''\n",
    "An explanation of the dynamics model and the buffers: \n",
    "\n",
    "We are training a model to produce p(x(t+1: t+F) | x(t-P: t), u(t-P: t), u(t+1: t+F-1)) \n",
    "\n",
    "I.e.: predict the future F frames, \n",
    "      given the past P frames, past P actions, and the future F-1 actions \n",
    "      \n",
    "This can be viewed as unrolling the classic p(x(t+1) | x(t), u(t)) dynamics model. \n",
    "\n",
    "We have proposed this 'unrolled dynamics' problem formulation for the following reasons:\n",
    "\n",
    "1) Conditioning on longer sequences to capture the effects of repeated actions, reduce discretization errors\n",
    "2) Improved training due to longer sequences. Single-step visual models have a tendency to \"copy\" prev. frame\n",
    "   as the output\n",
    "3) Be able to visualize a future state trajectory based on a proposed set of actions.\n",
    "   ---> this has applications in control planning, and can be useful to a human operator/co-worker, or for\n",
    "        another system to check for safety hazards, etc. Plus makes a sick demo (\"this is what the robot is\n",
    "        thinking\") lol. \n",
    "        \n",
    "PREV_FRAMES_BUFFER ---> input, this is x(t-P: t)         (size P,W,H,3*num_cams)\n",
    "PREV_ACTION_BUFFER ---> input, this is u(t-P: t)         (size P,A)\n",
    "FUTURE_FRAMES_BUFFER ---> output, this is x(t+1: t+F)    (size F,W,H,3*num_cams)\n",
    "FUTURE_ACTION_BUFFER ---> input, this is u(t+1: t+F-1)   (size F-1,A)           [if this was output, we'd be doing control]\n",
    "\n",
    "(note: visual inputs may be pre-processed with another pre-trained model)\n",
    "\n",
    "Other details/throughts: \n",
    "- ELU activations: to capture training efficiency of ReLU while reducing need for batch normalization\n",
    "- multi-frame unrolling vs. single frame p(x(t+1)|x(t),u(t)): unrolling is like doing batches of the single-frame\n",
    "  version, except the model can correlated (x,y)_t points in the batch   \n",
    "- The size of our conv layers (measured by # of filters per layer) creates a \"bottleneck\": form a more data-efficient\n",
    "  representation of the past information, combine with future action information, and then generate the larger tensor\n",
    "  of future predicted frames. \n",
    "\n",
    "'''\n",
    "# Can use part of pre-trained VGG model to seed features with reasonable features: (used online during training)  \n",
    "#vgg_preprocessor = CNN_Utils.GetVGGModel(\"block1_pool\", CAM_W, CAM_H, print_timing=1) \n",
    "# ^ Note: not using for this experiment (creates too many feature maps per camera for current hardware)\n",
    "\n",
    "# Model input #1: past frames\n",
    "input_prev_frames_raw = Input(shape=(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), name='input_prev_frames_raw')\n",
    "# below: some layers to learn what information is important from the past\n",
    "input_prev_frames = Convolution2D(CONVP1_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames_raw)\n",
    "input_prev_frames = Convolution2D(CONVP2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames)\n",
    "input_prev_frames = Convolution2D(CONVP3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames) \n",
    "\n",
    "# Model input #1: past actions \n",
    "input_prev_actions_raw = Input(shape=(NUM_PAST_FRAMES*ACTION_LEN,), name='input_prev_actions_raw')\n",
    "# Below: project and reshape, so we can merge inputs later: \n",
    "input_prev_actions_p = Dense(output_dim=CAM_W*CAM_H*1, activation='elu')(input_prev_actions_raw)\n",
    "input_prev_actions_p_r = Reshape((CAM_W, CAM_H, 1), name='input_prev_actions_p_r')(input_prev_actions_p)\n",
    "\n",
    "# Model input #3: future actions\n",
    "input_future_actions_raw = Input(shape=((NUM_FUTURE_FRAMES-1)*ACTION_LEN,), name='input_future_actions_raw')\n",
    "# format for convenience, to let us 'pick off' actions and sequentially predict the next frame logically\n",
    "# Below: these parameters gather information for an action pertaining to how it affects a future state\n",
    "D_A1 = Dense(output_dim=D_DENSE1, activation='elu', name='D_A1')(input_future_actions_raw)\n",
    "D_A2 = Dense(output_dim=D_DENSE2, activation='elu', name='D_A2')(D_A1)\n",
    "D_A3 = Dense(output_dim=D_DENSE3, activation='elu', name='D_A3')(D_A2)\n",
    "future_action_branch = Reshape((CAM_W, CAM_H, 1), name='action_branch_r')(D_A3)\n",
    "\n",
    "# MERGE PAST INPUTS: \n",
    "merged_input = merge([input_prev_frames, input_prev_actions_p_r], \n",
    "                      mode='concat', concat_axis=3, name='merged_prev_inputs')\n",
    "# ^ so now past frames, info from past actions has been merged into a tensor that is size e.g.:\n",
    "# [W, H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES/8 + 1] (e.g. == 16 for 10 past frames)\n",
    "\n",
    "# Pre-process merged inputs that contain past information:\n",
    "# This is meant to form a more efficient representation to be used in predicting future frames given proposed actions:\n",
    "merged_input = Convolution2D(CONV1_FEAT_MAPS, 7, 7, activation='elu', border_mode='same', name='merged_lvl_1')(merged_input)\n",
    "merged_input = Convolution2D(CONV2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='merged_lvl_2')(merged_input)\n",
    "merged_input = Convolution2D(CONV3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='merged_lvl_3')(merged_input)\n",
    "\n",
    "# Now merge with information about future actions: \n",
    "merged_input = merge([merged_input, future_action_branch], \n",
    "                      mode='concat', concat_axis=3, name='merged_efficient_inputs_all')\n",
    "# ^ This will have size [W, H, NUM_PAST_FRAMES + 1], e.g. [128, 128, 11]\n",
    "\n",
    "# Conv layers for predicting the next frames given all relevant data: \n",
    "Out_C1 = Convolution2D(CONVF1_FEAT_MAPS, 7, 7, activation='elu', border_mode='same', name='Out_C1')(merged_input)\n",
    "Out_C2 = Convolution2D(CONVF2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C2')(Out_C1)\n",
    "Out_C3 = Convolution2D(CONVF3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C3')(Out_C2)\n",
    "Out_C4 = Convolution2D(CONVF4_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C4')(Out_C3)\n",
    "# ^ Out_C4 is a final output layer, spits out a frame (defined as WxHx3*num_cams*num_future_frames)\n",
    "\n",
    "# Define the full model structure: \n",
    "model_inputs = [input_prev_frames_raw, input_prev_actions_raw, input_future_actions_raw]\n",
    "# Review of input dimensions: (caller of train/predict must expand_dims to achieve shapes)\n",
    "# input_prev_frames_raw: [W, H, 3*NUM_CAMS*NUM_PAST_FRAMES] e.g. [1, 128, 128, 120]\n",
    "# input_prev_actions_raw: [NUM_PAST_FRAMES*ACTION_LEN,] e.g. [1, 70] \n",
    "# input_future_actions_raw: [(NUM_FUTURE_FRAMES-1)*ACTION_LEN,] e.g. [1, 63]\n",
    "model_outputs = [Out_C4] \n",
    "# Target output data: \n",
    "# Out_C4: [W, H, 3*NUM_CAMS*NUM_FUTURE_FRAMES], e.g. [1, 128, 128, 120]\n",
    "UDM_model = Model(input=model_inputs, output=model_outputs)\n",
    "\n",
    "UDM_optimizer = RMSprop(lr=0.005, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "UDM_model.compile(loss='binary_crossentropy', optimizer=UDM_optimizer) \n",
    "# ^ TODO: custom loss function more appropriate for sequence of similar images...   \n",
    "\n",
    "UDM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### INITIALIZATION, SETUP OF MODEL #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################## Globals for training params ####################################\n",
    "\n",
    "global TOTAL_TRAINING_ITRS\n",
    "global SAVE_CHECKPOINT_ITRS\n",
    "global NUM_DEMONSTRATIONS\n",
    "global CURR_DEMONSTRATION\n",
    "global LENGTH_CURR_DEMONSTRATION # e.g. current demo we're looking at is 230 timesteps\n",
    "global T_CURR_DEMONSTRATION      # and e.g. we're currently on timestep 87\n",
    "global PERCENT_TRAIN             # percent of data used for training vs. valudation\n",
    "global DEMONSTRATION_FOLDERS\n",
    "global TRAINING_FOLDERS\n",
    "global TESTING_FOLDERS\n",
    "global NUM_TRAINING_DEMONSTRATIONS\n",
    "global NUM_TESTING_DEMONSTRATIONS\n",
    "global IMAGE_COMPARE_CHECKPOINT\n",
    "\n",
    "global PREV_FRAMES_BUFFER\n",
    "global PREV_ACTION_BUFFER\n",
    "global FUTURE_FRAMES_BUFFER\n",
    "global FUTURE_ACTION_BUFFER\n",
    "global MODEL_LOSS_BUFFER\n",
    "\n",
    "PREV_FRAMES_BUFFER = np.zeros((NUM_PAST_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "PREV_ACTION_BUFFER = np.zeros((NUM_PAST_FRAMES, ACTION_LEN))\n",
    "FUTURE_FRAMES_BUFFER = np.zeros((NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "FUTURE_ACTION_BUFFER = np.zeros((NUM_FUTURE_FRAMES-1, ACTION_LEN))\n",
    "\n",
    "TOTAL_TRAINING_ITRS = 100000\n",
    "SAVE_CHECKPOINT_ITRS = 10\n",
    "IMAGE_COMPARE_CHECKPOINT = 5\n",
    "CURR_DEMONSTRATION = -1 # start on the first one, loop to another one each itr\n",
    "LENGTH_CURR_DEMONSTRATION = -1\n",
    "PERCENT_TRAIN = 0.75\n",
    "MODEL_LOSS_BUFFER = np.zeros((10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running program...\n",
      "u_sequence_1486408000621\n",
      "u_sequence_1486420877768\n",
      "u_sequence_1486421372155\n",
      "u_sequence_1486422003939\n",
      "u_sequence_1486423780289\n",
      "u_sequence_1486424226636\n",
      "u_sequence_1486424664609\n",
      "u_sequence_1486425113072\n",
      "u_sequence_1486425723665\n",
      "u_sequence_1486426394928\n",
      "u_sequence_1486483126414\n",
      "u_sequence_1486483372904\n",
      "u_sequence_1486483607589\n",
      "u_sequence_1486483842933\n",
      "u_sequence_1486484146973\n",
      "u_sequence_1486484663068\n",
      "u_sequence_1486485509467\n",
      "u_sequence_1486485832231\n",
      "u_sequence_1486486076841\n",
      "u_sequence_1486486803219\n",
      "15\n",
      "5\n",
      "20\n",
      "(222, 128, 128, 12)\n",
      "(222, 7)\n",
      "\n",
      "===== Training on robot sample run #:7 with num timesteps: 222\n",
      "\n",
      "Loaded data for timestep 159 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 0 / timestep: 159\n",
      "Model update time: 15.225 secs\n",
      "Current Model Loss: 0.315351\n",
      "[[ 0.3153511]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 2.11 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905729187738097.png\n",
      "Model save checkpoint, itr: 0\n",
      "Model save time: 4.859 secs\n",
      "Loaded data for timestep 40 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 1 / timestep: 40\n",
      "Model update time: 5.424 secs\n",
      "Current Model Loss: 5.64835\n",
      "[[ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 53 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 2 / timestep: 53\n",
      "Model update time: 5.666 secs\n",
      "Current Model Loss: 5.799\n",
      "[[ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 68 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 3 / timestep: 68\n",
      "Model update time: 5.388 secs\n",
      "Current Model Loss: 2.03157\n",
      "[[ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 98 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 4 / timestep: 98\n",
      "Model update time: 5.802 secs\n",
      "Current Model Loss: 1.60563\n",
      "[[ 1.60563278]\n",
      " [ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 91 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 5 / timestep: 91\n",
      "Model update time: 6.053 secs\n",
      "Current Model Loss: 1.65379\n",
      "[[ 1.65379143]\n",
      " [ 1.60563278]\n",
      " [ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.152 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905729550777698.png\n",
      "Loaded data for timestep 117 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 6 / timestep: 117\n",
      "Model update time: 5.896 secs\n",
      "Current Model Loss: 1.60255\n",
      "[[ 1.60255373]\n",
      " [ 1.65379143]\n",
      " [ 1.60563278]\n",
      " [ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 140 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 7 / timestep: 140\n",
      "Model update time: 5.607 secs\n",
      "Current Model Loss: 1.52099\n",
      "[[ 1.52099311]\n",
      " [ 1.60255373]\n",
      " [ 1.65379143]\n",
      " [ 1.60563278]\n",
      " [ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 46 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 8 / timestep: 46\n",
      "Model update time: 5.378 secs\n",
      "Current Model Loss: 1.5716\n",
      "[[ 1.57160389]\n",
      " [ 1.52099311]\n",
      " [ 1.60255373]\n",
      " [ 1.65379143]\n",
      " [ 1.60563278]\n",
      " [ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 184 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 9 / timestep: 184\n",
      "Model update time: 5.495 secs\n",
      "Current Model Loss: 0.709171\n",
      "[[ 0.7091707 ]\n",
      " [ 1.57160389]\n",
      " [ 1.52099311]\n",
      " [ 1.60255373]\n",
      " [ 1.65379143]\n",
      " [ 1.60563278]\n",
      " [ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]\n",
      " [ 0.3153511 ]]\n",
      "\n",
      "\n",
      "Loaded data for timestep 171 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 10 / timestep: 171\n",
      "Model update time: 6.582 secs\n",
      "Current Model Loss: 1.08434\n",
      "[[ 1.08433723]\n",
      " [ 0.7091707 ]\n",
      " [ 1.57160389]\n",
      " [ 1.52099311]\n",
      " [ 1.60255373]\n",
      " [ 1.65379143]\n",
      " [ 1.60563278]\n",
      " [ 2.03156853]\n",
      " [ 5.79900122]\n",
      " [ 5.64834976]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.008 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905729857675881.png\n",
      "Model save checkpoint, itr: 10\n"
     ]
    }
   ],
   "source": [
    "########################################## START THE TRAINING #######################################\n",
    "\n",
    "print(\"Running program...\")\n",
    "\n",
    "DEMONSTRATION_FOLDERS = Video_Utils.GetFoldersForRuns()\n",
    "\n",
    "NUM_DEMONSTRATIONS = len(DEMONSTRATION_FOLDERS)\n",
    "\n",
    "for f in DEMONSTRATION_FOLDERS:\n",
    "    print(f)\n",
    "\n",
    "# Separate into training and testing data based on the number of recordings (folders) we have: \n",
    "random.shuffle(DEMONSTRATION_FOLDERS)    \n",
    "lim_separate = round(NUM_DEMONSTRATIONS * PERCENT_TRAIN)    \n",
    "TRAINING_FOLDERS = DEMONSTRATION_FOLDERS[0:lim_separate]\n",
    "TESTING_FOLDERS = DEMONSTRATION_FOLDERS[lim_separate:]\n",
    "NUM_TRAINING_DEMONSTRATIONS = len(TRAINING_FOLDERS)\n",
    "NUM_TESTING_DEMONSTRATIONS = len(TESTING_FOLDERS)\n",
    "\n",
    "print(NUM_TRAINING_DEMONSTRATIONS); print(NUM_TESTING_DEMONSTRATIONS); print(NUM_DEMONSTRATIONS) # a + b = c\n",
    "\n",
    "    \n",
    "\n",
    "# Data flow process: we train on entire folder (sample run of a robot) before moving on to the next to \n",
    "# amortize the time required to load that folder's training data into RAM (multiple seconds). For a dynamics\n",
    "# model this should be perfectly acceptable because the dynamics to be learned are ideally the *same* between\n",
    "# different sample runs recorded on the (simulated) robot. \n",
    "itrs = 0\n",
    "while itrs < TOTAL_TRAINING_ITRS:\n",
    "    \n",
    "    # Choose which expert demonstration we're using: \n",
    "    CURR_DEMONSTRATION = randint(0,NUM_TRAINING_DEMONSTRATIONS-1)\n",
    "\n",
    "    frames, actions = Video_Utils.LoadFramesActionsFromFolder(TRAINING_FOLDERS[CURR_DEMONSTRATION], CAM_W, CAM_H, CAM_C*NUM_CAMS, ACTION_LEN)\n",
    "    # ^ about 5 secs\n",
    "    \n",
    "    print(frames.shape)\n",
    "    print(actions.shape)\n",
    "    \n",
    "    LENGTH_CURR_DEMONSTRATION = frames.shape[0] # number of timesteps for this demonstration\n",
    "    \n",
    "    # New demonstration, reset the relevant data buffers:\n",
    "    PREV_FRAMES_BUFFER = np.zeros((NUM_PAST_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "    PREV_ACTION_BUFFER = np.zeros((NUM_PAST_FRAMES, ACTION_LEN))\n",
    "    FUTURE_FRAMES_BUFFER = np.zeros((NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "    FUTURE_ACTION_BUFFER = np.zeros((NUM_FUTURE_FRAMES-1, ACTION_LEN))\n",
    "    \n",
    "    print(\"\\n===== Training on robot sample run #:\"+str(CURR_DEMONSTRATION)+\" with num timesteps: \"+str(LENGTH_CURR_DEMONSTRATION)+\"\\n\")\n",
    "    \n",
    "    for i in range(0, round(LENGTH_CURR_DEMONSTRATION/NUM_FUTURE_FRAMES)): \n",
    "        # only sample a limited number from each demo before moving on to next demo. \n",
    "        t = randint(0,LENGTH_CURR_DEMONSTRATION-1)\t# now we have random shuffle training within a demo.\n",
    "            \n",
    "        ##### STEP 1: Load Buffers with Data #####    \n",
    "        ms1 = time.time()*1000.0 \n",
    "        # Past data: (up to and including x(t), and u(t) - which causes x(t+1), etc)\n",
    "        if (t >= NUM_PAST_FRAMES): # regular, in-bounds\n",
    "            PREV_FRAMES_BUFFER = frames[t-NUM_PAST_FRAMES:t, :] \n",
    "            PREV_ACTION_BUFFER_BUFFER = actions[t-NUM_PAST_FRAMES:t, :]\n",
    "        else: # t is less than past frames, need to concat zeros at beginning\n",
    "            LIM1 = np.abs(t - NUM_PAST_FRAMES) \n",
    "            PREV_FRAMES_BUFFER = np.concatenate((np.zeros((LIM1, CAM_W, CAM_H, CAM_C*NUM_CAMS)), frames[0:t, :]), axis=0) \n",
    "            PREV_ACTIONS_BUFFER = np.concatenate((np.zeros((LIM1, ACTION_LEN)), actions[0:t, :]), axis=0)\n",
    "        # Future Data: (include 1 less future action than frame to retain logical p(x(t+1)|x(t),u(t)) structure)\n",
    "        if (t <= LENGTH_CURR_DEMONSTRATION - NUM_FUTURE_FRAMES - 1):\n",
    "            FUTURE_FRAMES_BUFFER = frames[t:t+NUM_FUTURE_FRAMES, :]\n",
    "            FUTURE_ACTIONS_BUFFER = actions[t:t+NUM_FUTURE_FRAMES-1, :]\n",
    "        else: # running off the end\n",
    "            LIM2 = t + NUM_FUTURE_FRAMES - LENGTH_CURR_DEMONSTRATION \n",
    "            # ^ We need to repeat last frame this many times / add this many zero actions\n",
    "            FUTURE_FRAMES_BUFFER = frames[t:LENGTH_CURR_DEMONSTRATION, :]\n",
    "            last_frame = np.expand_dims(frames[LENGTH_CURR_DEMONSTRATION-1], axis=0)\n",
    "            for j in range(0,LIM2):\n",
    "                FUTURE_FRAMES_BUFFER = np.concatenate((FUTURE_FRAMES_BUFFER, last_frame), axis=0) \n",
    "            FUTURE_ACTIONS_BUFFER = np.concatenate((actions[t:LENGTH_CURR_DEMONSTRATION, :], np.zeros((LIM2, ACTION_LEN))), axis=0)\n",
    "        ms2 = time.time()*1000.0\n",
    "        print(\"\\nLoaded data for timestep \"+str(t)+\" in \"+str(round((ms2-ms1)/1, 3))+\" msecs\\n\")\n",
    "        \n",
    "        #print(PREV_FRAMES_BUFFER.shape)\n",
    "        #print(PREV_ACTION_BUFFER.shape)\n",
    "        #print(FUTURE_FRAMES_BUFFER.shape)\n",
    "        #print(FUTURE_ACTION_BUFFER.shape)\n",
    "        \n",
    "        ##### STEP 2: Train on Batch of Data Gathered Above #####\n",
    "        ms1 = time.time()*1000.0\n",
    "        # inputs: [input_prev_frames_raw, input_prev_actions_raw, input_future_actions_raw]\n",
    "        # outputs/training targets: [future_frames]\n",
    "        model_loss = UDM_model.train_on_batch(\n",
    "                     [np.expand_dims(PREV_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0), \n",
    "                      np.expand_dims(PREV_ACTION_BUFFER.reshape(-1), axis=0), \n",
    "                      np.expand_dims(FUTURE_ACTION_BUFFER.reshape(-1), axis=0)], # <--- inputs \n",
    "                     [np.expand_dims(FUTURE_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0)])\n",
    "        \n",
    "        ms2 = time.time()*1000.0\n",
    "        print(\"===================================================\")\n",
    "        print(\"iter: \" + str(itrs) + \" / timestep: \"+str(t))\n",
    "        print(\"Model update time: \"+str(round((ms2-ms1)/1000, 3)) + ' secs')\n",
    "        print(\"Current Model Loss: \"+str(model_loss))\n",
    "        MODEL_LOSS_BUFFER = np.roll(MODEL_LOSS_BUFFER, 1); MODEL_LOSS_BUFFER[0] = model_loss\n",
    "        print(MODEL_LOSS_BUFFER); print(\"\\n\")\n",
    "        \n",
    "        ##### STEP 3: CHECKPOINTS #####\n",
    "        \n",
    "        if (itrs % IMAGE_COMPARE_CHECKPOINT == 0 ): # View how the model is doing\n",
    "            print(\"Image compare checkpoint...\")\n",
    "            timestamp = str(time.time()).replace(\".\",\"\")\n",
    "            \n",
    "            # Get the generated future frames: \n",
    "            ms1 = time.time()*1000.0\n",
    "            GEN_FUTURE_FRAMES = UDM_model.predict(\n",
    "                                [np.expand_dims(PREV_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0), \n",
    "                                 np.expand_dims(PREV_ACTION_BUFFER.reshape(-1), axis=0), \n",
    "                                 np.expand_dims(FUTURE_ACTION_BUFFER.reshape(-1), axis=0)])\n",
    "            GEN_FUTURE_FRAMES = GEN_FUTURE_FRAMES[0]\n",
    "            ms2 = time.time()*1000.0\n",
    "            print(\"Model prediction time: \"+str(round((ms2-ms1)/1000, 3)) +' secs'+' for '+str(NUM_FUTURE_FRAMES)+' future frames.')\n",
    "            \n",
    "            imgs_gen = Video_Utils.ViewFutureFrames(GEN_FUTURE_FRAMES.reshape(NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "            imgs_split = np.zeros((30, NUM_FUTURE_FRAMES*CAM_W, 3)) # goes in between to separate real/generated images\n",
    "            imgs_gt = Video_Utils.ViewFutureFrames(FUTURE_FRAMES_BUFFER)\n",
    "            #print(imgs_gt.shape); print(imgs_split.shape); print(imgs_gen.shape);\n",
    "            imgs_compare = np.concatenate((imgs_gt, imgs_split, imgs_gen),axis=0)\n",
    "            #print(imgs_compare.shape)\n",
    "            #cv2.imshow('image',imgs_compare) \n",
    "            #cv2.waitKey(0)\n",
    "            img_filename = 'sample_'+timestamp+'.png' \n",
    "            cv2.imwrite(img_filename,(imgs_compare*255 + 127.5))\n",
    "            print(\"Wrote new image sample checkpoint at: \"+img_filename)\n",
    "            \n",
    "        if (itrs % SAVE_CHECKPOINT_ITRS == 0): # save progress\n",
    "            print(\"Model save checkpoint, itr: \"+str(itrs))\n",
    "            ms1 = time.time()*1000.0\n",
    "            timestamp = str(time.time()).replace(\".\",\"\")\n",
    "            mean_recent_loss = round(np.mean(MODEL_LOSS_BUFFER), 4)\n",
    "            model_str_name = 'UDM_weights_'+str(mean_recent_loss)+'.h5' # pro-tip: manually re-name after each run... \n",
    "            UDM_model.save(model_str_name)\n",
    "            ms2 = time.time()*1000.0\n",
    "            print(\"Model save time: \"+str(round((ms2-ms1)/1000, 3)) +' secs')\n",
    "\n",
    "        itrs = itrs + 1 # don't forget to increment total training itrs counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.04925164]\n",
      " [-0.01916969]\n",
      " [ 0.30727009]\n",
      " [-0.65106343]\n",
      " [-0.52131578]\n",
      " [ 0.56544691]\n",
      " [-0.04157708]\n",
      " [-0.10824067]\n",
      " [ 0.12230494]\n",
      " [-1.27076234]\n",
      " [-1.37299658]\n",
      " [-2.49650858]\n",
      " [ 0.07053112]\n",
      " [-0.52609533]\n",
      " [-1.22834526]\n",
      " [ 1.08385355]\n",
      " [-0.46551641]\n",
      " [-0.21067492]\n",
      " [ 0.65173058]\n",
      " [-0.39597755]]\n",
      "\n",
      "\n",
      "[[-0.04157708]\n",
      " [-0.10824067]\n",
      " [ 0.12230494]\n",
      " [-1.27076234]]\n",
      "\n",
      "\n",
      "[[-1.37299658]\n",
      " [-2.49650858]\n",
      " [ 0.07053112]\n",
      " [-0.52609533]\n",
      " [-1.22834526]]\n",
      "[-0.39597755]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
